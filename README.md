
# Virtual Fence: Person Detection, Tracking, and Counting  

This repository implements a **Virtual Fence system** to detect, track, and count people crossing predefined regions in video streams.  

We evaluate and compare **three major approaches**:  
- **YOLO (You Only Look Once)** â€“ fast real-time detection.  
- **Faster R-CNN (ResNet50-FPN v2)** â€“ high accuracy, strong region proposals.  
- **Vision-Language Model (VLM, OWLv2)** â€“ prompt-based open-vocabulary detection.  

The system supports:  
âœ”ï¸ **Data preprocessing**  
âœ”ï¸ **Training & fine-tuning**  
âœ”ï¸ **Inference on videos**  
âœ”ï¸ **Counting across zones**  
âœ”ï¸ **Benchmarking & visualization**  
âœ”ï¸ **ONNX export for deployment**  

---

## ğŸ“‚ Repository Structure  

```
virtual-fence/
â”‚
â”œâ”€â”€ datasets/                      # Datasets
â”‚   â”œâ”€â”€ person_rf/                 # Roboflow COCO Person-only dataset
â”‚   â”œâ”€â”€ penn_fudan/                # Penn-Fudan pedestrian dataset
â”‚   â””â”€â”€ custom/                    # Custom collected dataset
â”‚
â”œâ”€â”€ virtual_fence/                 # Main package
â”‚   â”œâ”€â”€ detectors/                 # YOLO, Faster R-CNN, VLM classes
â”‚   â”œâ”€â”€ trackers/                  # SORT tracker
â”‚   â””â”€â”€  utils/                     # Helpers 
â”‚
â”œâ”€â”€ scripts/                       # Entry points
â”‚   â”œâ”€â”€ inference.py               # Run inference
â”‚   â”œâ”€â”€ benchmark.py               # Run benchmark
â”‚   â”œâ”€â”€ train.py                   # Training/fine-tuning
â”‚   â”œâ”€â”€ export_yolo_onnx.py             # ONNX export
â”‚   â”œâ”€â”€ eval_ap50.py
â”‚   â”œâ”€â”€ inference_onnx.py
â”‚   â”œâ”€â”€ convert_penndufan.py
â”‚   â””â”€â”€ spilt_yolo_dataset.py
â”‚
â”œâ”€â”€ configs/                       # Dataset/hyperparameter configs
â”œâ”€â”€ outputs/                       # Output videos & benchmark logs
â”œâ”€â”€ runs/                          # training proccess and weights  
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## ğŸ—‚ï¸ Datasets  

### 1. COCO Person-Only Dataset (Roboflow)  
Download from: [Roboflow COCO Person Dataset](https://universe.roboflow.com/computer-vision-6lifs/coco-dataset-limited--person-only-hrvsw)  

Extract into:  
```
datasets/person_rf/
â”‚â”€â”€ images/
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ val/
â”‚   â””â”€â”€ test/
â”‚â”€â”€ labels/
â”‚â”€â”€ person.yaml
```

### 2. Penn-Fudan Pedestrian Dataset  
Download from: [Penn-Fudan Dataset](https://www.cis.upenn.edu/~jshi/ped_html/)  

Extract into:  
```
datasets/penn_fudan/
â”‚â”€â”€ PNGImages/         # Raw images
â”‚â”€â”€ PedMasks/          # Segmentation masks
â”‚â”€â”€ Annotations/       # Bounding box annotations
```

### 3. Custom Dataset  
Collected from video recordings and annotated with **LabelImg**.  

Structure:  
```
datasets/custom/
â”‚â”€â”€ images/
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ val/
â”‚â”€â”€ labels/
â”‚â”€â”€ custom.yaml
```

---

## â–¶ï¸ Inference  

```bash
python -m scripts.inference     --input input.mp4     --output outputs/yolo.mp4     --zone 200,100,700,500     --detector yolo     --tracker sort     --weights yolov8n.pt
```

Arguments:  
- `--input`: Input video file.  
- `--output`: Output path.  
- `--zone`: Virtual fence zone (x1,y1,x2,y2).  
- `--detector`: `yolo`, `rcnn`, `vlm`.  
- `--tracker`: Tracker type (e.g., `sort`).  
- `--weights`: Model weights file.  

---

## ğŸ‹ï¸ Training  

### YOLO / RT-DETR  
```bash
yolo detect train     model=rtdetr-l.pt     data=datasets/person_rf/person.yaml     epochs=80     imgsz=1280     batch=8     workers=0     device=0     name=rtdetr_l_person_rf
```

### Faster R-CNN  
```bash
python -m scripts.train     --detector rcnn     --dataset datasets/person_rf/person.yaml     --epochs 50     --batch-size 4
```

---

## ğŸ“¦ ONNX Export  

- YOLO:
  ```bash
  yolo export model=yolov8s.pt format=onnx
  ```

- Faster R-CNN:
  ```bash
  python -m scripts.export_onnx --detector rcnn --weights model.pth --output rcnn.onnx
  ```

ONNX allows deployment to TensorRT, OpenVINO, or ONNXRuntime.  

---

## ğŸ“ˆ Benchmarking  

```bash
python -m scripts.benchmark --detectors yolo rcnn vlm     --weights yolov8n.pt yolov8s.pt yolov8m.pt rtdetr-l.pt
```

---

## ğŸ“Š Benchmark Results  

| Detector | Weights | Time (s) | Count | Output Video |
|----------|---------|----------|-------|--------------|
| VLM | default | 1099.5 | 91 | outputs\bench\vlm.mp4 |
| YOLO | yolov8n.pt | 95.2 | 60 | outputs\bench\yolo-yolov8n-1efd56.mp4 |
| YOLO | yolov8s.pt | 73.1 | 79 | outputs\bench\yolo-yolov8s-1b1b99.mp4 |
| YOLO | yolov8m.pt | 120.3 | 58 | outputs\bench\yolo-yolov8m-db76f6.mp4 |
| YOLO | rtdetr-l.pt | 195.3 | 118 | outputs\bench\yolo-rtdetr-l-e1f79e.mp4 |
| YOLO | runs/detect/y8n_pennfudan3/weights/best.pt | 68.5 | 23 | outputs\bench\yolo-best-25ae78.mp4 |
| YOLO | runs/detect/rtdetr_l_person_rf_16503/weights/best.pt | 262.3 | 361 | outputs\bench\yolo-best-f3c1ea.mp4 |
| RCNN | default | 430.5 | 125 | outputs\bench\rcnn.mp4 |


### Visualizations  

**1. Inference Time per Model**  
![Inference Time](outputs/benchmark_time.png)  

**2. Person Count Comparison**  
![Counts](outputs/benchmark_counts.png)  

**3. Accuracy vs Speed (approximation)**  
![Accuracy vs FPS](outputs/benchmark_accuracy_vs_fps.png)  

---

## âš–ï¸ Why These Models?  

- **YOLO** was chosen for real-time deployment.  
- **Faster R-CNN** was chosen for high accuracy (uses Region Proposal Networks).  
- **VLM (OWLv2)** was chosen for open-vocabulary, prompt-based detection.  

---

## âš™ï¸ Configuration Files

The `configs/` directory contains YAML configuration files used by training scripts.

- **configs/dataset_yolo.yaml**  
  Dataset definition for YOLO training.  
  Specifies training/validation paths and the single class `person`.  
  **Usage:**
  ```bash
  python scripts/train_yolo.py --data configs/dataset_yolo.yaml --epochs 50 --weights yolov8n.pt
  ```

- **configs/hyp_person.yaml**  
  Hyperparameters for YOLO training, including augmentation, learning rate, momentum, warmup, etc.  
  **Usage (override default hyperparameters):**
  ```bash
  python scripts/train_yolo.py --data configs/dataset_yolo.yaml --hyp configs/hyp_person.yaml
  ```

- **configs/person.yaml**  
  Alternative dataset config (YOLO/RT-DETR style) pointing to train/val/test folders and declaring the class list.  
  **Usage:**
  ```bash
  yolo detect train model=rtdetr-l.pt data=configs/person.yaml epochs=80 imgsz=1280
  ```

These configs make experiments reproducible and allow quick changes without editing the training scripts.

---

## ğŸ“œ License  

MIT License.  
